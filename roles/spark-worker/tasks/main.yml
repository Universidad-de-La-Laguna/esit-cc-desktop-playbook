- name: "Crea directorios de apps y datos"
  file:
    path: "{{ item }}"
    state: directory
    mode: '0777'
  with_items:
    - /opt/spark/apps
    - /opt/spark/data

- name: "Instalar Docker SDK for Python"
  pip:
    name: docker

- name: "Run spark-worker container"
  docker_container:
    name: spark-worker-{{ inventory_hostname | regex_replace('^([^\.]+)\..*$', '\1') }}
    image: ccesitull/spark-worker:2.4.3
    restart_policy: always
    env:
      SPARK_MASTER: "spark://10.6.7.12:7077"
      SPARK_WORKER_CORES: "1"
      SPARK_WORKER_MEMORY: "1G"
      SPARK_DRIVER_MEMORY: "1G"
      SPARK_EXECUTOR_MEMORY: "256m"
      HDFS_DATANODE_IP: "{{ HDFS_DATANODE_IP }}"
    network_mode: host
    volumes:
      - /opt/spark/apps:/opt/spark-apps
      - /opt/spark/data:/opt/spark-data

- name: "Copia la aplicaci√≥n de ejemplo"
  copy: src={{ item.src }} dest={{ item.dest }}
  with_items:
    - { src: '{{ EXAMPLE_APP }}', dest: '/opt/spark/apps/'}
    - { src: 'ejemplo.txt', dest: '/opt/spark/data/'}

- name: "Copia el script de lanzamiento"
  copy: src={{ item.src }} dest={{ item.dest }} mode={{ item.mode }}
  with_items:
    - { src: 'submitApp.sh', dest: '/opt/spark/', mode: '0755' }

- name: Bash aliases | Add aliases
  lineinfile:
    dest: "/etc/bash.bashrc"
    create: yes
    mode: 0644
    line: 'alias hdfs="docker run --rm -e HDFS_CONF_dfs_client_use_datanode_hostname=true --add-host datanode:{{ HDFS_DATANODE_IP }} ccesitull/hdfs-client hdfs"'
    regexp: "^alias hdfs="
